# -*- coding: utf-8 -*-
"""ALY6020_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ho7ggwkZAeS59HKY1rzEm6lsl61gUynZ

## Module Project
"""

import pandas as pd
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from collections import Counter

"""### DATA Import"""

df = pd.read_csv('car.csv')
df.head()

df.tail()

"""## Visualization & Understanding Dataset"""

pip install pandas_profiling

def missing_values(df):
    missing_number = df.isnull().sum().sort_values(ascending=False)
    missing_percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)
    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])
    return missing_values[missing_values['Missing_Number']>0]

def first_looking(df):
    print(colored("Shape:", attrs=['bold']), df.shape,'\n',
          colored('-'*79, 'red', attrs=['bold']),
          colored("\nInfo:\n", attrs=['bold']), sep='')
    print(df.info(), '\n',
          colored('-'*79, 'red', attrs=['bold']), sep='')
    print(colored("Number of Uniques:\n", attrs=['bold']), df.nunique(),'\n',
          colored('-'*79, 'red', attrs=['bold']), sep='')
    print(colored("Missing Values:\n", attrs=['bold']), missing_values(df),'\n',
          colored('-'*79, 'red', attrs=['bold']), sep='')
    print(colored("All Columns:", attrs=['bold']), list(df.columns),'\n',
          colored('-'*79, 'red', attrs=['bold']), sep='')

    df.columns= df.columns.str.lower().str.replace('&', '_').str.replace(' ', '_')

    print(colored("Columns after rename:", attrs=['bold']), list(df.columns),'\n',
              colored('-'*79, 'red', attrs=['bold']), sep='')

pip install colorama

pip install termcolor

import colorama
from colorama import Fore, Style  # maakes strings colored
from termcolor import colored

missing_values(df)

first_looking(df)

df.describe()

import pandas_profiling

df.profile_report()

"""### Data Cleansing"""

df['horsepower'].unique()

df.replace({"?":None}, inplace=True)

df['horsepower'].unique()

df['horsepower'] = pd.to_numeric(df['horsepower'], errors='coerce')

df.describe()

print(df['horsepower'].median())
print(df['horsepower'].mean())
print(df['horsepower'].mode().iloc[0])

missing_data = df[df['horsepower'].isna()]
complete_data = df[~df['horsepower'].isna()]

import statsmodels.api as sm

y = complete_data['horsepower']
X = complete_data.drop('horsepower', axis=1)

X.reset_index(drop=True, inplace=True)
y.reset_index(drop=True, inplace=True)

X = sm.add_constant(X)
y = pd.to_numeric(y, errors='coerce')

y

model = sm.OLS(y, X).fit()

X2 = missing_data.drop('horsepower', axis=1)
missing_data_with_const = sm.add_constant(X2)

predicted_data = model.predict(missing_data_with_const)
predicted_data

df.loc[df['horsepower'].isna(), 'horsepower'] = predicted_data

df['horsepower']

df.describe()

"""## Data Visualization"""

import seaborn as sns

df1 = df.copy()

# mpg histogram
#distribution of capital_gain
plt.figure(figsize=(10, 5))
sns.histplot(x=df1['mpg'], kde=True)

plt.title("Distribution of mpg")

# cylinderso histogram
#distribution of cylinders
plt.figure(figsize=(10, 5))
sns.histplot(x=df1['cylinders'], binwidth=1, binrange=(2.5, 9.5))

plt.title("Distribution of cylinders")

# displacement histogram
#distribution of displacement
plt.figure(figsize=(10, 5))
sns.histplot(x=df1['displacement'], kde=True)

plt.title("Distribution of displacement")

# weight histogram
#distribution of weight
plt.figure(figsize=(10, 5))
sns.histplot(x=df1['weight'], kde=True)

plt.title("Distribution of weight")

# acceleration histogram
#distribution of acceleration
plt.figure(figsize=(10, 5))
sns.histplot(x=df1['acceleration'], kde=True)

plt.title("Distribution of acceleration")

# model_year histogram
#distribution of model_year
plt.figure(figsize=(10, 5))
sns.histplot(x=df1['model_year'], binwidth=1, binrange=(69.5, 82.5), kde=True)

plt.title("Distribution of model_year")

counts = df1['us_made'].value_counts()

# Extract labels and sizes for the pie chart
labels = counts.index.tolist()
sizes = counts.values

# Create a pie chart
plt.figure(figsize=(6, 6))
plt.pie(sizes, labels=[f'{label} ({count})' for label, count in zip(labels, sizes)], autopct='%.1f%%', startangle=140)

# Display the pie chart
plt.title('us_made')  # Optional: Add a title to the chart
plt.show()



num_vars = df.drop(['us_made'], axis=1)
num_vars

sns.set_palette('colorblind')
sns.pairplot(data=df)

"""#### Categorical value ANOVA test"""

df_anova=df[['us_made', 'mpg']]
df_anova

from scipy.stats import f_oneway

# Running the one-way anova test between CarPrice and FuelTypes
# Assumption(H0) is that FuelType and CarPrices are NOT correlated

# Finds out the Prices data for each FuelType as a list
for i in df_anova.columns:
    if i != 'mpg':
        CategoryGroupLists = df_anova.groupby(i)['mpg'].apply(list).values
        AnovaResults = f_oneway(*CategoryGroupLists)
        print(f'P-Value for Anova with {i} is:', AnovaResults.pvalue)

"""### Data visualization"""

import seaborn as sns

car_corr_matrix=num_vars.corr()

plt.figure(figsize=(20, 20))
sns.heatmap(car_corr_matrix, cmap='coolwarm', annot=True)

"""### Best Subset method"""

import time
import itertools

y=df.mpg
y

X=df.drop(['mpg', 'us_made'], axis=1).astype('float64')
X

def processSubset(feature_set):
    # Fit model on feature_set and calculate RSS
    X_subset = sm.add_constant(X[list(feature_set)])
    model = sm.OLS(y,X_subset)
    regr = model.fit()
    RSS = ((regr.predict(X_subset) - y) ** 2).sum()
    return {"model":regr, "RSS":RSS}

def getBest(k):

    tic = time.time()

    results = []

    for combo in itertools.combinations(X.columns, k):
        results.append(processSubset(combo))

    # Wrap everything up in a nice dataframe
    models = pd.DataFrame(results)

    # Choose the model with the highest RSS
    best_model = models.loc[models['RSS'].argmin()]

    toc = time.time()
    print("Processed", models.shape[0], "models on", k, "predictors in", (toc-tic), "seconds.")

    # Return the best model, along with some other useful information about the model
    return best_model

# Could take quite awhile to complete...

models_best = pd.DataFrame(columns=["RSS", "model"])

tic = time.time()
for i in range(1,7):
    models_best.loc[i] = getBest(i)

toc = time.time()
print("Total elapsed time:", (toc-tic), "seconds.")

# Gets the second element from each row ('model') and pulls out its rsquared attribute
models_best.apply(lambda row: row[1].rsquared, axis=1)

print(models_best.loc[6, "model"].summary())

"""### Checking multicollinearity"""

df2=df.copy()

from patsy import dmatrices
from statsmodels.stats.outliers_influence import variance_inflation_factor

#find design matrix for regression model using 'rating' as response variable
y, X = dmatrices('mpg ~ cylinders + displacement + horsepower + weight + acceleration + model_year', data=df2, return_type='dataframe')

#create DataFrame to hold VIF values
vif_df = pd.DataFrame()
vif_df['variable'] = X.columns

#calculate VIF for each predictor variable
vif_df['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

#view VIF for each predictor variable
print(vif_df)

vif_df = vif_df[vif_df['variable'] != 'Intercept']
vif_df

plt.figure(figsize=(10, 6))
plt.bar(vif_df['variable'], vif_df['VIF'], color='skyblue')
plt.axhline(y=5, color='red', linestyle='--', label='VIF Threshold (5)')

plt.xlabel('Predictor Variables')
plt.ylabel('VIF Values')
plt.title('VIF Values for Predictor Variables')
plt.xticks(rotation=45, ha='right')
plt.legend()
plt.tight_layout()

plt.show()

"""### Multicollinearity check 2"""

print(models_best.loc[2, "model"].summary())

#find design matrix for regression model using 'rating' as response variable
y, X = dmatrices('mpg ~ weight + model_year', data=df2, return_type='dataframe')

#create DataFrame to hold VIF values
vif_df = pd.DataFrame()
vif_df['variable'] = X.columns

#calculate VIF for each predictor variable
vif_df['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

#view VIF for each predictor variable
print(vif_df)

vif_df = vif_df[vif_df['variable'] != 'Intercept']
vif_df

plt.figure(figsize=(10, 6))
plt.bar(vif_df['variable'], vif_df['VIF'], color='skyblue')
plt.axhline(y=5, color='red', linestyle='--', label='VIF Threshold (5)')

plt.xlabel('Predictor Variables')
plt.ylabel('VIF Values')
plt.title('VIF Values for Predictor Variables')
plt.xticks(rotation=45, ha='right')
plt.legend()
plt.tight_layout()

plt.show()

"""#### Checking proper number of attributes"""

plt.figure(figsize=(20,10))
plt.rcParams.update({'font.size': 18, 'lines.markersize': 10})

# Set up a 2x2 grid so we can look at 4 plots at once
plt.subplot(2, 2, 1)

# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.
# The argmax() function can be used to identify the location of the maximum point of a vector
plt.plot(models_best["RSS"])
plt.xlabel('# Predictors')
plt.ylabel('RSS')

# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.
# The argmax() function can be used to identify the location of the maximum point of a vector

rsquared_adj = models_best.apply(lambda row: row[1].rsquared_adj, axis=1)

plt.subplot(2, 2, 2)
plt.plot(rsquared_adj)
plt.xlabel('# Predictors')
plt.ylabel('adjusted rsquared')

# We'll do the same for AIC and BIC, this time looking for the models with the SMALLEST statistic
aic = models_best.apply(lambda row: row[1].aic, axis=1)

plt.subplot(2, 2, 3)
plt.plot(aic)
plt.xlabel('# Predictors')
plt.ylabel('AIC')

bic = models_best.apply(lambda row: row[1].bic, axis=1)

plt.subplot(2, 2, 4)
plt.plot(bic)
plt.xlabel('# Predictors')
plt.ylabel('BIC')

"""### with dummy varialbes"""

df1

y=df1[['mpg']]
x=df1.iloc[:, 1:9]
x=df1.drop(['mpg', 'cylinders', 'displacement', 'horsepower', 'acceleration'], axis=1)
x

y=y.mpg
y

x = sm.add_constant(x)

"""### Final Regression"""

y=df1[['mpg']]
x=df1.iloc[:, 1:9]
x=df1.drop(['mpg', 'cylinders', 'displacement', 'horsepower', 'acceleration'], axis=1)
x



result=sm.OLS(y, x).fit()

print(result.summary())

"""#### with normalization"""

x.drop(['us_made'], axis=1)

from sklearn.preprocessing import MinMaxScaler

min_max_scaler = MinMaxScaler().fit(x)
X_norm = min_max_scaler.transform(x)
X_norm

min_max_scaler = MinMaxScaler().fit(y)
y_norm = min_max_scaler.transform(y)
y_norm

y_norm_df = pd.DataFrame(y_norm, columns=y.columns)
y=y_norm_df.mpg
y



X_norm_df = pd.DataFrame(X_norm, columns=x.columns)
X_norm_df

x = X_norm_df
x = pd.concat([x, df1[['us_made']]], axis=1)

x

x = sm.add_constant(x)

result=sm.OLS(y, x).fit()

print(result.summary())

"""### checking assumptions"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# %config InlineBackend.figure_format ='retina'
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.stats.api as sms
sns.set_style('darkgrid')
sns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)

def linearity_test(model, y):
    '''
    Function for visually inspecting the assumption of linearity in a linear regression model.
    It plots observed vs. predicted values and residuals vs. predicted values.

    Args:
    * model - fitted OLS model from statsmodels
    * y - observed values
    '''
    fitted_vals = model.predict()
    resids = model.resid

    fig, ax = plt.subplots(1,2)

    sns.regplot(x=fitted_vals, y=y, lowess=True, ax=ax[0], line_kws={'color': 'red'})
    ax[0].set_title('Observed vs. Predicted Values', fontsize=16)
    ax[0].set(xlabel='Predicted', ylabel='Observed')

    sns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[1], line_kws={'color': 'red'})
    ax[1].set_title('Residuals vs. Predicted Values', fontsize=16)
    ax[1].set(xlabel='Predicted', ylabel='Residuals')

linearity_test(result, y)

predicted_values = result.fittedvalues
residuals = result.resid

predicted_values

residuals

data = pd.DataFrame({'Predicted Values': predicted_values, 'Residuals': residuals})
data

plt.figure(figsize=(8, 6))
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residuals vs Fitted Values Plot for Homoscedasticity Check')
sns.scatterplot(x='Predicted Values', y='Residuals', data=data, color='blue', edgecolor='k')
plt.axhline(y=0, color='r', linestyle='--')
plt.show()

"""### Shapiro Test"""

from scipy import stats

x = data['Residuals']
x1 = residuals

x

res = stats.shapiro(x)

res.statistic

plt.figure(figsize=(10, 5))
sns.histplot(x=data['Residuals'], kde=True, binwidth=0.05)

plt.title("Distribution of residuals")

def statistic(x):
    # Get only the `shapiro` statistic; ignore its p-value
    return stats.shapiro(x).statistic
ref = stats.monte_carlo_test(x, stats.norm.rvs, statistic,
                             alternative='less')
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(8, 5))
def plot(ax):  # we'll re-use this
    ax.hist(ref.null_distribution, density=True)
    ax.set_title("Shapiro-Wilk Test Null Distribution \n"
                 "(Monte Carlo Approximation, 11 Observations)")
    ax.set_xlabel("statistic")
    ax.set_ylabel("probability density")
plot(ax)
plt.show()

sns.boxplot(data=df1, x="us_made", y="mpg")